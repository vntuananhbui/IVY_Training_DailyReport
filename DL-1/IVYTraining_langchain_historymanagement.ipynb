{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./.conda/lib/python3.11/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "gemini_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = gemini_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-google-genai in ./.conda/lib/python3.11/site-packages (2.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./.conda/lib/python3.11/site-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in ./.conda/lib/python3.11/site-packages (from langchain-google-genai) (0.8.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in ./.conda/lib/python3.11/site-packages (from langchain-google-genai) (0.3.29)\n",
      "Requirement already satisfied: pydantic<3,>=2 in ./.conda/lib/python3.11/site-packages (from langchain-google-genai) (2.10.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in ./.conda/lib/python3.11/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.10)\n",
      "Requirement already satisfied: google-api-core in ./.conda/lib/python3.11/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.24.0)\n",
      "Requirement already satisfied: google-api-python-client in ./.conda/lib/python3.11/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.159.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in ./.conda/lib/python3.11/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.37.0)\n",
      "Requirement already satisfied: protobuf in ./.conda/lib/python3.11/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.29.3)\n",
      "Requirement already satisfied: tqdm in ./.conda/lib/python3.11/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/macintosh/.local/lib/python3.11/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in ./.conda/lib/python3.11/site-packages (from google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.25.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.2.10)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/macintosh/.local/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (24.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.conda/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (9.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.conda/lib/python3.11/site-packages (from pydantic<3,>=2->langchain-google-genai) (2.27.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in ./.conda/lib/python3.11/site-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.66.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in ./.conda/lib/python3.11/site-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.conda/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.conda/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.conda/lib/python3.11/site-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.conda/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.conda/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.10.14)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.conda/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in ./.conda/lib/python3.11/site-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in ./.conda/lib/python3.11/site-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in ./.conda/lib/python3.11/site-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.1.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in ./.conda/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.69.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in ./.conda/lib/python3.11/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.69.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in ./.conda/lib/python3.11/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.2.1)\n",
      "Requirement already satisfied: anyio in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (4.8.0)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (0.14.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in ./.conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.conda/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-google-genai) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macintosh/TA-DOCUMENT/StudyZone/FPT_WORK/IVY Training/IVY DEV/DailyReport/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I just said \"I love programming\" in French.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "ai_msg = chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Translate from English to French: I love programming.\"\n",
    "            ),\n",
    "            AIMessage(content=\"J'adore la programmation.\"),\n",
    "            HumanMessage(content=\"What did you just say?\"),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraph\n",
      "  Downloading langgraph-0.2.62-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in ./.conda/lib/python3.11/site-packages (from langgraph) (0.3.29)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.4 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.0.10-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.1.51-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.conda/lib/python3.11/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.conda/lib/python3.11/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in ./.conda/lib/python3.11/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.2.10)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/macintosh/.local/lib/python3.11/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in ./.conda/lib/python3.11/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.conda/lib/python3.11/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/macintosh/.local/lib/python3.11/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (4.12.2)\n",
      "Collecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph)\n",
      "  Downloading msgpack-1.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: httpx>=0.25.2 in ./.conda/lib/python3.11/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in ./.conda/lib/python3.11/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.14)\n",
      "Requirement already satisfied: anyio in ./.conda/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.8.0)\n",
      "Requirement already satisfied: certifi in ./.conda/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in ./.conda/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.conda/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.conda/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.conda/lib/python3.11/site-packages (from langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.15,!=0.3.16,!=0.3.17,!=0.3.18,!=0.3.19,!=0.3.2,!=0.3.20,!=0.3.21,!=0.3.22,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langgraph) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.conda/lib/python3.11/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
      "Downloading langgraph-0.2.62-py3-none-any.whl (138 kB)\n",
      "Downloading langgraph_checkpoint-2.0.10-py3-none-any.whl (37 kB)\n",
      "Downloading langgraph_sdk-0.1.51-py3-none-any.whl (44 kB)\n",
      "Downloading msgpack-1.1.0-cp311-cp311-macosx_11_0_arm64.whl (81 kB)\n",
      "Installing collected packages: msgpack, langgraph-sdk, langgraph-checkpoint, langgraph\n",
      "Successfully installed langgraph-0.2.62 langgraph-checkpoint-2.0.10 langgraph-sdk-0.1.51 msgpack-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. \"\n",
    "        \"Answer all questions to the best of your ability.\"\n",
    "    )\n",
    "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the node and edge\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# Add simple in-memory checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='2b241221-f730-47a5-a6a1-1f8f1930f905'),\n",
       "  AIMessage(content=\"The most straightforward translation is:  **J'aime la programmation.**\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-0aea9680-c271-4ef3-aec3-d398921dc0df-0', usage_metadata={'input_tokens': 25, 'output_tokens': 15, 'total_tokens': 40, 'input_token_details': {'cache_read': 0}})]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Translate to French: I love programming.\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='dc6bcc0a-88e3-44be-8add-b0a0de1f1772'),\n",
       "  AIMessage(content='You asked me: \"What did I just ask you?\"', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-5def1070-c323-4d5a-9241-f80fa6597a83-0', usage_metadata={'input_tokens': 24, 'output_tokens': 13, 'total_tokens': 37, 'input_token_details': {'cache_read': 0}})]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What did I just ask you?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"2\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimming Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What did I just ask you?', additional_kwargs={}, response_metadata={}, id='dc6bcc0a-88e3-44be-8add-b0a0de1f1772'),\n",
       "  AIMessage(content='You asked me: \"What did I just ask you?\"', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-5def1070-c323-4d5a-9241-f80fa6597a83-0', usage_metadata={'input_tokens': 24, 'output_tokens': 13, 'total_tokens': 37, 'input_token_details': {'cache_read': 0}}),\n",
       "  HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='b57b58a8-1197-450b-a889-b9bcf3688c8b'),\n",
       "  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='2150d04c-3fd6-4db9-8f22-c7b58c20c693'),\n",
       "  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='00f611e3-7329-4b7c-a607-60fca3f9991a'),\n",
       "  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='b09aaf58-3040-49f2-8fce-8ee8e7b6b972'),\n",
       "  HumanMessage(content=\"What's my name?\", additional_kwargs={}, response_metadata={}, id='37252fdc-a57c-43d4-b049-134a71173fbc'),\n",
       "  AIMessage(content='Your name is Nemo.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-6d9f20af-4ca7-406a-8193-d7606cfa9d3a-0', usage_metadata={'input_tokens': 66, 'output_tokens': 6, 'total_tokens': 72, 'input_token_details': {'cache_read': 0}})]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_ephemeral_chat_history = [\n",
    "    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
    "    AIMessage(content=\"Hello!\"),\n",
    "    HumanMessage(content=\"How are you today?\"),\n",
    "    AIMessage(content=\"Fine thanks!\"),\n",
    "]\n",
    "\n",
    "app.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history\n",
    "        + [HumanMessage(content=\"What's my name?\")]\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": \"2\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define trimmer\n",
    "# count each message as 1 \"token\" (token_counter=len) and keep only the last two messages\n",
    "trimmer = trim_messages(strategy=\"last\", max_tokens=2, token_counter=len)\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. \"\n",
    "        \"Answer all questions to the best of your ability.\"\n",
    "    )\n",
    "    messages = [SystemMessage(content=system_prompt)] + trimmed_messages\n",
    "    response = llm.invoke(messages)\n",
    "    print(\"trim_messages\", trimmed_messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the node and edge\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# Add simple in-memory checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trim_messages [AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='b09aaf58-3040-49f2-8fce-8ee8e7b6b972'), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='d60874e0-75c7-4624-840b-65e88f3ed0fb')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Hey there! I'm Nemo.\", additional_kwargs={}, response_metadata={}, id='b57b58a8-1197-450b-a889-b9bcf3688c8b'),\n",
       "  AIMessage(content='Hello!', additional_kwargs={}, response_metadata={}, id='2150d04c-3fd6-4db9-8f22-c7b58c20c693'),\n",
       "  HumanMessage(content='How are you today?', additional_kwargs={}, response_metadata={}, id='00f611e3-7329-4b7c-a607-60fca3f9991a'),\n",
       "  AIMessage(content='Fine thanks!', additional_kwargs={}, response_metadata={}, id='b09aaf58-3040-49f2-8fce-8ee8e7b6b972'),\n",
       "  HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}, id='d60874e0-75c7-4624-840b-65e88f3ed0fb'),\n",
       "  AIMessage(content='I do not know your name.  I have no access to personal information about you unless you explicitly provide it to me.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-deba2567-d553-41f5-bb0c-6127d362b9c0-0', usage_metadata={'input_tokens': 26, 'output_tokens': 26, 'total_tokens': 52, 'input_token_details': {'cache_read': 0}})]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history\n",
    "        + [HumanMessage(content=\"What is my name?\")]\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": \"2\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_ephemeral_chat_history = [\n",
    "    HumanMessage(content=\"Hey there! I'm Nemo.\"),\n",
    "    AIMessage(content=\"Hello!\"),\n",
    "    HumanMessage(content=\"How are you today?\"),\n",
    "    AIMessage(content=\"Fine thanks!\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, RemoveMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. \"\n",
    "        \"Answer all questions to the best of your ability. \"\n",
    "        \"The provided chat history includes a summary of the earlier conversation.\"\n",
    "    )\n",
    "    system_message = SystemMessage(content=system_prompt)\n",
    "    message_history = state[\"messages\"][:-1]  # exclude the most recent user input\n",
    "    # Summarize the messages if the chat history reaches a certain size\n",
    "    if len(message_history) >= 4:\n",
    "        last_human_message = state[\"messages\"][-1]\n",
    "        # Invoke the model to generate conversation summary\n",
    "        summary_prompt = (\n",
    "            \"Distill the above chat messages into a single summary message. \"\n",
    "            \"Include as many specific details as you can.\"\n",
    "        )\n",
    "        summary_message = llm.invoke(\n",
    "            message_history + [HumanMessage(content=summary_prompt)]\n",
    "        )\n",
    "\n",
    "        # Delete messages that we no longer want to show up\n",
    "        delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"]]\n",
    "        # Re-add user message\n",
    "        human_message = HumanMessage(content=last_human_message.content)\n",
    "        # Call the model with summary & response\n",
    "        response = llm.invoke([system_message, summary_message, human_message])\n",
    "        message_updates = [summary_message, human_message, response] + delete_messages\n",
    "    else:\n",
    "        message_updates = llm.invoke([system_message] + state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": message_updates}\n",
    "\n",
    "\n",
    "# Define the node and edge\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# Add simple in-memory checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content='Nemo initiated a conversation by saying \"Hey there! I\\'m Nemo.\"  I responded with \"Hello!\" and then Nemo asked \"How are you today?\", to which I replied \"Fine thanks!\".', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-f6151420-21da-4e94-a1f6-3df243937f89-0', usage_metadata={'input_tokens': 44, 'output_tokens': 41, 'total_tokens': 85, 'input_token_details': {'cache_read': 0}}),\n",
       "  HumanMessage(content='What did I say my name was?', additional_kwargs={}, response_metadata={}, id='3ba04d91-9bcf-49fb-ab8c-936a60643609'),\n",
       "  AIMessage(content='You said your name was Nemo.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-7b2a9782-9626-41c4-9053-d4b1eb633c32-0', usage_metadata={'input_tokens': 78, 'output_tokens': 8, 'total_tokens': 86, 'input_token_details': {'cache_read': 0}})]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\n",
    "    {\n",
    "        \"messages\": demo_ephemeral_chat_history\n",
    "        + [HumanMessage(\"What did I say my name was?\")]\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": \"4\"}},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
